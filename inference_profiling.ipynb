{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4162f6e",
   "metadata": {},
   "source": [
    "# LLaDA Inference Profiling on HumanEval\n",
    "\n",
    "This notebook runs inference on the LLaDA model using the HumanEval dataset and collects wall time statistics for profiling.\n",
    "\n",
    "## Beneficial Statistics for Profiling\n",
    "For inference profiling, especially with diffusion models like LLaDA, the following statistics are beneficial:\n",
    "1.  **Total Wall Time (Latency)**: The total time taken to generate a complete solution.\n",
    "2.  **Time Per Step**: Since LLaDA is a diffusion model, measuring the time taken per diffusion step is crucial.\n",
    "3.  **Throughput**: If batching is used, samples per second.\n",
    "4.  **Memory Usage**: Peak GPU memory consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ead82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_HOME'] = './hf_models/'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Import local generate function\n",
    "from generate import generate\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66410e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: GSAI-ML/LLaDA-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:186: UserWarning: The `resume_download` argument is deprecated and ignored in `hf_hub_download`. Downloads always resume whenever possible.\n",
      "  `use_auth_token` is passed to a function, the `use_auth_token` value is passed\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- configuration_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct:\n",
      "- modeling_llada.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]Cancellation requested; stopping current tasks.\n",
      "Downloading shards:   0%|          | 0/6 [04:24<?, ?it/s]Cancellation requested; stopping current tasks.\n",
      "Downloading shards:   0%|          | 0/6 [04:24<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/file_download.py:571\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, tqdm_class, _tqdm_bar)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mxet_get\u001b[39m(\n\u001b[32m    530\u001b[39m     *,\n\u001b[32m    531\u001b[39m     incomplete_path: Path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    536\u001b[39m     _tqdm_bar: Optional[tqdm] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    537\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Download a file using Xet storage service.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m        incomplete_path (`Path`):\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m            The path to the file to download.\u001b[39;00m\n\u001b[32m    544\u001b[39m \u001b[33;03m        xet_file_data (`XetFileData`):\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m            The file metadata needed to make the request to the xet storage service.\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m        headers (`Dict[str, str]`):\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m            The headers to send to the xet storage service.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m        expected_size (`int`, *optional*):\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[33;03m            The expected size of the file to download. If set, the download will raise an error if the size of the\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[33;03m            received content is different from the expected one.\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[33;03m        displayed_filename (`str`, *optional*):\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[33;03m            The filename of the file that is being downloaded. Value is used only to display a nice progress bar. If\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[33;03m            not set, the filename is guessed from the URL or the `Content-Disposition` header.\u001b[39;00m\n\u001b[32m    554\u001b[39m \n\u001b[32m    555\u001b[39m \u001b[33;03m    **How it works:**\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m        The file download system uses Xet storage, which is a content-addressable storage system that breaks files into chunks\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[33;03m        for efficient storage and transfer.\u001b[39;00m\n\u001b[32m    558\u001b[39m \n\u001b[32m    559\u001b[39m \u001b[33;03m        `hf_xet.download_files` manages downloading files by:\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[33;03m        - Taking a list of files to download (each with its unique content hash)\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[33;03m        - Connecting to a storage server (CAS server) that knows how files are chunked\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m        - Using authentication to ensure secure access\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[33;03m        - Providing progress updates during download\u001b[39;00m\n\u001b[32m    564\u001b[39m \n\u001b[32m    565\u001b[39m \u001b[33;03m        Authentication works by regularly refreshing access tokens through `refresh_xet_connection_info` to maintain a valid\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[33;03m        connection to the storage server.\u001b[39;00m\n\u001b[32m    567\u001b[39m \n\u001b[32m    568\u001b[39m \u001b[33;03m        The download process works like this:\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[33;03m        1. Create a local cache folder at `~/.cache/huggingface/xet/chunk-cache` to store reusable file chunks\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03m        2. Download files in parallel:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m \u001b[33;03m            2.1. Prepare to write the file to disk\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m            2.2. Ask the server \"how is this file split into chunks?\" using the file's unique hash\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m                The server responds with:\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[33;03m                - Which chunks make up the complete file\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03m                - Where each chunk can be downloaded from\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[33;03m            2.3. For each needed chunk:\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m                - Checks if we already have it in our local cache\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[33;03m                - If not, download it from cloud storage (S3)\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[33;03m                - Save it to cache for future use\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[33;03m                - Assemble the chunks in order to recreate the original file\u001b[39;00m\n\u001b[32m    581\u001b[39m \n\u001b[32m    582\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m.to(device).eval()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Ensure padding side is left for generation\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.padding_side != \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:556\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    555\u001b[39m         \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    560\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/transformers/modeling_utils.py:3264\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m   3263\u001b[39m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3264\u001b[39m     resolved_archive_file, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3273\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3275\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3276\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3280\u001b[39m     is_safetensors_available()\n\u001b[32m   3281\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m   3282\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3283\u001b[39m ):\n\u001b[32m   3284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/transformers/utils/hub.py:1038\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc=\u001b[33m\"\u001b[39m\u001b[33mDownloading shards\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m         cached_filename = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1052\u001b[39m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[32m   1053\u001b[39m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/transformers/utils/hub.py:398\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    395\u001b[39m user_agent = http_user_agent(user_agent)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    413\u001b[39m     resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:89\u001b[39m, in \u001b[36m_inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_hf_hub_args\u001b[39m(fn: CallableT) -> CallableT:\n\u001b[32m     43\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate values received as argument for any public method of `huggingface_hub`.\u001b[39;00m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[33;03m    The goal of this decorator is to harmonize validation of arguments reused\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m    everywhere. By default, all defined validators are tested.\u001b[39;00m\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \u001b[33;03m    Validators:\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m        - [`~utils.validate_repo_id`]: `repo_id` must be `\"repo_name\"`\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m          or `\"namespace/repo_name\"`. Namespace is a username or an organization.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m        - [`~utils.smoothly_deprecate_use_auth_token`]: Use `token` instead of\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m          `use_auth_token` (only if `use_auth_token` is not expected by the decorated\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m          function - in practice, always the case in `huggingface_hub`).\u001b[39;00m\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    ```py\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m    >>> from huggingface_hub.utils import validate_hf_hub_args\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33;03m    >>> @validate_hf_hub_args\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    ... def my_cool_method(repo_id: str):\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    ...     print(repo_id)\u001b[39;00m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[33;03m    >>> my_cool_method(repo_id=\"valid_repo_id\")\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    valid_repo_id\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33;03m    >>> my_cool_method(\"other..repo..id\")\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.\u001b[39;00m\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m \u001b[33;03m    >>> my_cool_method(repo_id=\"other..repo..id\")\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m    >>> @validate_hf_hub_args\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    ... def my_cool_auth_method(token: str):\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    ...     print(token)\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[33;03m    >>> my_cool_auth_method(token=\"a token\")\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m    \"a token\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[33;03m    >>> my_cool_auth_method(use_auth_token=\"a use_auth_token\")\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m    \"a use_auth_token\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m     82\u001b[39m \u001b[33;03m    >>> my_cool_auth_method(token=\"a token\", use_auth_token=\"a use_auth_token\")\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    UserWarning: Both `token` and `use_auth_token` are passed (...)\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    \"a token\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m     86\u001b[39m \n\u001b[32m     87\u001b[39m \u001b[33;03m    Raises:\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m        [`~utils.HFValidationError`]:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[33;03m            If an input is not valid.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# TODO: add an argument to opt-out validation for specific argument?\u001b[39;00m\n\u001b[32m     92\u001b[39m     signature = inspect.signature(fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[32m   1008\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m   1009\u001b[39m         cache_dir=cache_dir,\n\u001b[32m   1010\u001b[39m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[32m   1011\u001b[39m         repo_id=repo_id,\n\u001b[32m   1012\u001b[39m         filename=filename,\n\u001b[32m   1013\u001b[39m         repo_type=repo_type,\n\u001b[32m   1014\u001b[39m         revision=revision,\n\u001b[32m   1015\u001b[39m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[32m   1016\u001b[39m         endpoint=endpoint,\n\u001b[32m   1017\u001b[39m         etag_timeout=etag_timeout,\n\u001b[32m   1018\u001b[39m         headers=hf_headers,\n\u001b[32m   1019\u001b[39m         proxies=proxies,\n\u001b[32m   1020\u001b[39m         token=token,\n\u001b[32m   1021\u001b[39m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[32m   1022\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1023\u001b[39m         force_download=force_download,\n\u001b[32m   1024\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/file_download.py:1200\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1181\u001b[39m             _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_hf_hub_download_to_local_dir\u001b[39m(\n\u001b[32m   1187\u001b[39m     *,\n\u001b[32m   1188\u001b[39m     \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m   1189\u001b[39m     local_dir: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   1190\u001b[39m     \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[32m   1191\u001b[39m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1192\u001b[39m     repo_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1193\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1194\u001b[39m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1195\u001b[39m     \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[32m   1196\u001b[39m     endpoint: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   1197\u001b[39m     etag_timeout: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m   1198\u001b[39m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   1199\u001b[39m     proxies: Optional[Dict],\n\u001b[32m-> \u001b[39m\u001b[32m1200\u001b[39m     token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m   1201\u001b[39m     \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[32m   1202\u001b[39m     cache_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   1203\u001b[39m     force_download: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m   1204\u001b[39m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m   1205\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1206\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download a given file to a local folder, if not already present.\u001b[39;00m\n\u001b[32m   1207\u001b[39m \n\u001b[32m   1208\u001b[39m \u001b[33;03m    Method should not be called directly. Please use `hf_hub_download` instead.\u001b[39;00m\n\u001b[32m   1209\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1210\u001b[39m     \u001b[38;5;66;03m# Some Windows versions do not allow for paths longer than 255 characters.\u001b[39;00m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m# In this case, we must specify it as an extended path by using the \"\\\\?\\\" prefix.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/file_download.py:1791\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, headers, expected_size, filename, force_download, etag, xet_file_data, tqdm_class)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_copy_no_matter_what\u001b[39m(src: \u001b[38;5;28mstr\u001b[39m, dst: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1790\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Copy file from src to dst.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m \n\u001b[32m   1792\u001b[39m \u001b[33;03m    If `shutil.copy2` fails, fallback to `shutil.copyfile`.\u001b[39;00m\n\u001b[32m   1793\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1795\u001b[39m         \u001b[38;5;66;03m# Copy file with metadata and permission\u001b[39;00m\n\u001b[32m   1796\u001b[39m         \u001b[38;5;66;03m# Can fail e.g. if dst is an S3 mount\u001b[39;00m\n\u001b[32m   1797\u001b[39m         shutil.copy2(src, dst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/huggingface_hub/file_download.py:566\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, tqdm_class, _tqdm_bar)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mxet_get\u001b[39m(\n\u001b[32m    530\u001b[39m     *,\n\u001b[32m    531\u001b[39m     incomplete_path: Path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    536\u001b[39m     _tqdm_bar: Optional[tqdm] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    537\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[33;03m    Download a file using Xet storage service.\u001b[39;00m\n\u001b[32m    540\u001b[39m \n\u001b[32m    541\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m        incomplete_path (`Path`):\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m            The path to the file to download.\u001b[39;00m\n\u001b[32m    544\u001b[39m \u001b[33;03m        xet_file_data (`XetFileData`):\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m            The file metadata needed to make the request to the xet storage service.\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m        headers (`Dict[str, str]`):\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m            The headers to send to the xet storage service.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m        expected_size (`int`, *optional*):\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[33;03m            The expected size of the file to download. If set, the download will raise an error if the size of the\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[33;03m            received content is different from the expected one.\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[33;03m        displayed_filename (`str`, *optional*):\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[33;03m            The filename of the file that is being downloaded. Value is used only to display a nice progress bar. If\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[33;03m            not set, the filename is guessed from the URL or the `Content-Disposition` header.\u001b[39;00m\n\u001b[32m    554\u001b[39m \n\u001b[32m    555\u001b[39m \u001b[33;03m    **How it works:**\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m        The file download system uses Xet storage, which is a content-addressable storage system that breaks files into chunks\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[33;03m        for efficient storage and transfer.\u001b[39;00m\n\u001b[32m    558\u001b[39m \n\u001b[32m    559\u001b[39m \u001b[33;03m        `hf_xet.download_files` manages downloading files by:\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[33;03m        - Taking a list of files to download (each with its unique content hash)\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[33;03m        - Connecting to a storage server (CAS server) that knows how files are chunked\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m        - Using authentication to ensure secure access\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[33;03m        - Providing progress updates during download\u001b[39;00m\n\u001b[32m    564\u001b[39m \n\u001b[32m    565\u001b[39m \u001b[33;03m        Authentication works by regularly refreshing access tokens through `refresh_xet_connection_info` to maintain a valid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m \u001b[33;03m        connection to the storage server.\u001b[39;00m\n\u001b[32m    567\u001b[39m \n\u001b[32m    568\u001b[39m \u001b[33;03m        The download process works like this:\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[33;03m        1. Create a local cache folder at `~/.cache/huggingface/xet/chunk-cache` to store reusable file chunks\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03m        2. Download files in parallel:\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m            2.1. Prepare to write the file to disk\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m            2.2. Ask the server \"how is this file split into chunks?\" using the file's unique hash\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m                The server responds with:\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[33;03m                - Which chunks make up the complete file\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03m                - Where each chunk can be downloaded from\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[33;03m            2.3. For each needed chunk:\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m                - Checks if we already have it in our local cache\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[33;03m                - If not, download it from cloud storage (S3)\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[33;03m                - Save it to cache for future use\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[33;03m                - Assemble the chunks in order to recreate the original file\u001b[39;00m\n\u001b[32m    581\u001b[39m \n\u001b[32m    582\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    584\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_xet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyXetDownloadInfo, download_files  \u001b[38;5;66;03m# type: ignore[no-redef]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/autogen/lib/python3.12/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "# Using the Instruct model as per chat.py example\n",
    "model_id = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
    "\n",
    "# Ensure padding side is left for generation\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf80dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HumanEval Dataset\n",
    "print(\"Loading HumanEval dataset...\")\n",
    "ds = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "print(f\"Loaded {len(ds)} problems.\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample Problem:\")\n",
    "print(ds[0]['prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, prompt_text, steps=128, gen_length=128, block_length=32):\n",
    "    \"\"\"\n",
    "    Runs inference and measures wall time.\n",
    "    \"\"\"\n",
    "    # Prepare input\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = generate(\n",
    "            model, \n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            steps=steps, \n",
    "            gen_length=gen_length, \n",
    "            block_length=block_length, \n",
    "            temperature=0., \n",
    "            cfg_scale=0., \n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text, wall_time\n",
    "\n",
    "print(\"Inference function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e59ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on a Subset\n",
    "num_samples = 5  # Adjust as needed\n",
    "results = []\n",
    "\n",
    "print(f\"Running inference on first {num_samples} samples...\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    problem = ds[i]\n",
    "    prompt = problem['prompt']\n",
    "    task_id = problem['task_id']\n",
    "    \n",
    "    print(f\"Processing {task_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Using parameters from chat.py/generate.py examples\n",
    "        output, duration = run_inference(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            prompt, \n",
    "            steps=128, \n",
    "            gen_length=128, \n",
    "            block_length=32\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"wall_time\": duration,\n",
    "            \"output_length\": len(output),\n",
    "            \"output\": output\n",
    "        })\n",
    "        print(f\"  Time: {duration:.4f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "if not df_results.empty:\n",
    "    stats = {\n",
    "        \"Mean Latency\": df_results['wall_time'].mean(),\n",
    "        \"Median Latency\": df_results['wall_time'].median(),\n",
    "        \"Std Dev\": df_results['wall_time'].std(),\n",
    "        \"Min\": df_results['wall_time'].min(),\n",
    "        \"Max\": df_results['wall_time'].max(),\n",
    "        \"P95\": df_results['wall_time'].quantile(0.95),\n",
    "        \"P99\": df_results['wall_time'].quantile(0.99)\n",
    "    }\n",
    "\n",
    "    print(\"Wall Time Statistics (seconds):\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"No results to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de144847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Latency\n",
    "if not df_results.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_results['wall_time'], kde=True, bins=10)\n",
    "    plt.title('Inference Wall Time Distribution')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(df_results['wall_time'].mean(), color='r', linestyle='--', label=f\"Mean: {df_results['wall_time'].mean():.2f}s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
