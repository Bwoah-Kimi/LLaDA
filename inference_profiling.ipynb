{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4162f6e",
   "metadata": {},
   "source": [
    "# LLaDA Inference Profiling on HumanEval\n",
    "\n",
    "This notebook runs inference on the LLaDA model using the HumanEval dataset and collects wall time statistics for profiling.\n",
    "\n",
    "## Beneficial Statistics for Profiling\n",
    "For inference profiling, especially with diffusion models like LLaDA, the following statistics are beneficial:\n",
    "1.  **Total Wall Time (Latency)**: The total time taken to generate a complete solution.\n",
    "2.  **Time Per Step**: Since LLaDA is a diffusion model, measuring the time taken per diffusion step is crucial.\n",
    "3.  **Throughput**: If batching is used, samples per second.\n",
    "4.  **Memory Usage**: Peak GPU memory consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ead82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_HOME'] = '/root/LLaDA/hf_models/'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from generate import generate\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "cache_path = '/root/LLaDA/hf_models/hub'\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, cache_dir=cache_path, local_files_only=True).to(device).eval()\n",
    "\n",
    "# Ensure padding side is left for generation\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf80dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HumanEval Dataset\n",
    "print(\"Loading HumanEval dataset...\")\n",
    "dataset_path = '/root/LLaDA/hf_models/datasets/openai_humaneval'\n",
    "ds = load_dataset(path=dataset_path, split=\"test\")\n",
    "print(f\"Loaded {len(ds)} problems.\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample Problem:\")\n",
    "print(ds[0]['prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiled_generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0., cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    \"\"\"\n",
    "    A version of the generate function with internal profiling for steps and token masks.\n",
    "    \"\"\"\n",
    "    # Internal helper for getting blocks\n",
    "    def get_blocks(start, end, step):\n",
    "        if step == 0:\n",
    "            return []\n",
    "        return [i for i in range(start, end, step)] + [end]\n",
    "\n",
    "    x = prompt\n",
    "    \n",
    "    # Profiling data storage\n",
    "    step_timings = []\n",
    "    step_details = []\n",
    "\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    for i in get_blocks(0, gen_length, block_length):\n",
    "        block_start_time = time.perf_counter()\n",
    "        \n",
    "        # Extend sequence with masks for this block\n",
    "        prefix_len = x.shape[1]\n",
    "        x = torch.cat((x, torch.full((1, block_length), mask_id, dtype=torch.long).to(x.device)), dim=1)\n",
    "        \n",
    "        # Determine mask indices for the new block\n",
    "        mask_indices = torch.zeros(x.shape, dtype=torch.bool, device=x.device)\n",
    "        mask_indices[:, prefix_len:] = True\n",
    "\n",
    "        # Denoising loop for this block\n",
    "        for step in range(steps):\n",
    "            step_start = time.perf_counter()\n",
    "            \n",
    "            # 1. Model Forward Pass\n",
    "            if cfg_scale > 0:\n",
    "                # CFG logic omitted for brevity/speed in profiling unless requested, assuming cfg=0 based on notebook usage\n",
    "                logits = model(x).logits\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            # 2. Sampling\n",
    "            # Only look at the masked positions\n",
    "            current_masks = (x == mask_id)\n",
    "            \n",
    "            # Simple greedy/temperature sampling logic (simplified from standard generate for clarity)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                x0 = torch.multinomial(probs.view(-1, probs.shape[-1]), 1).view(x.shape)\n",
    "            else:\n",
    "                x0 = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # 3. Remasking Strategy\n",
    "            # Calculate confidence\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            confidence = torch.gather(probs, -1, x0.unsqueeze(-1)).squeeze(-1)\n",
    "            \n",
    "            # Apply remasking schedule\n",
    "            ratio = (steps - step - 1) / steps\n",
    "            \n",
    "            # Identify which tokens to keep vs remask\n",
    "            # We only care about the currently masked/generated part, but the tensor shape must match x\n",
    "            \n",
    "            # Create a mask for where we should re-mask based on confidence\n",
    "            # This logic mimics 'low_confidence' remasking\n",
    "            \n",
    "            # Get confidence only for the tokens we are currently generating (the block)\n",
    "            # Note: In LLaDA, we might be re-evaluating the whole sequence or just the block. \n",
    "            # Usually, the mask is applied to the whole sequence, but we only change tokens that were masks.\n",
    "            \n",
    "            # Find the N tokens with lowest confidence among those that were masks\n",
    "            # To do this correctly per sample in batch (batch=1 here):\n",
    "            \n",
    "            is_mask_position = mask_indices # The positions we are generating in this block\n",
    "            \n",
    "            # We temporarily update x with the predicted tokens x0\n",
    "            x_curr = torch.where(is_mask_position, x0, x)\n",
    "            \n",
    "            if step < steps - 1:\n",
    "                # Determine how many to remask\n",
    "                num_to_remask = int(block_length * ratio)\n",
    "                \n",
    "                # Filter confidence to only relevant positions\n",
    "                # Set confidence of non-mask positions to infinity so they aren't picked\n",
    "                conf_for_ranking = confidence.clone()\n",
    "                conf_for_ranking[~is_mask_position] = float('inf')\n",
    "                \n",
    "                # Find indices of lowest confidence\n",
    "                # We flatten to find topk (smallest)\n",
    "                \n",
    "                # Get the indices of the lowest confidence tokens in the block\n",
    "                # We need to re-mask 'num_to_remask' tokens.\n",
    "                \n",
    "                if num_to_remask > 0:\n",
    "                    # Get values and indices of lowest confidence\n",
    "                    # We only care about the last 'block_length' positions usually\n",
    "                    # But let's do it globally for the mask_indices\n",
    "                    \n",
    "                    # Mask out non-generated tokens\n",
    "                    masked_conf = torch.where(is_mask_position, confidence, torch.tensor(float('inf'), device=x.device))\n",
    "                    \n",
    "                    # Get indices of the k lowest confidence tokens\n",
    "                    # We flatten, find indices, then unflatten\n",
    "                    flat_conf = masked_conf.view(-1)\n",
    "                    _, remask_flat_indices = torch.topk(flat_conf, k=num_to_remask, largest=False)\n",
    "                    \n",
    "                    # Create a remask tensor\n",
    "                    remask_mask = torch.zeros_like(flat_conf, dtype=torch.bool)\n",
    "                    remask_mask[remask_flat_indices] = True\n",
    "                    remask_mask = remask_mask.view(x.shape)\n",
    "                    \n",
    "                    # Apply remasking\n",
    "                    x = torch.where(remask_mask, mask_id, x_curr)\n",
    "                    \n",
    "                    # Record details\n",
    "                    remasked_count = num_to_remask\n",
    "                    sampled_tokens = (~remask_mask & is_mask_position).sum().item()\n",
    "                else:\n",
    "                    x = x_curr\n",
    "                    remasked_count = 0\n",
    "                    sampled_tokens = is_mask_position.sum().item()\n",
    "            else:\n",
    "                # Last step, keep everything\n",
    "                x = x_curr\n",
    "                remasked_count = 0\n",
    "                sampled_tokens = is_mask_position.sum().item()\n",
    "\n",
    "            step_end = time.perf_counter()\n",
    "            step_duration = step_end - step_start\n",
    "            \n",
    "            step_timings.append(step_duration)\n",
    "            step_details.append({\n",
    "                \"block_idx\": i,\n",
    "                \"step_idx\": step,\n",
    "                \"duration\": step_duration,\n",
    "                \"remasked_count\": remasked_count,\n",
    "                \"sampled_count\": sampled_tokens,\n",
    "                \"total_generated_so_far\": (x != mask_id).sum().item() - prefix_len\n",
    "            })\n",
    "\n",
    "    total_end = time.perf_counter()\n",
    "    \n",
    "    return x, step_details\n",
    "\n",
    "def run_inference(model, tokenizer, prompt_text, steps=64, gen_length=64, block_length=32):\n",
    "    \"\"\"\n",
    "    Runs inference and measures wall time with detailed profiling.\n",
    "    \"\"\"\n",
    "    # Prepare input\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out, step_details = profiled_generate(\n",
    "            model, \n",
    "            input_ids, \n",
    "            steps=steps, \n",
    "            gen_length=gen_length, \n",
    "            block_length=block_length, \n",
    "            temperature=0., \n",
    "            cfg_scale=0., \n",
    "            remasking='low_confidence'\n",
    "        )\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text, wall_time, step_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e59ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on a Subset\n",
    "num_samples = 5\n",
    "results = []\n",
    "all_step_details = []\n",
    "\n",
    "print(f\"Running inference on first {num_samples} samples...\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    problem = ds[i]\n",
    "    prompt = problem['prompt']\n",
    "    task_id = problem['task_id']\n",
    "    \n",
    "    print(f\"Processing {task_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Using parameters from chat.py/generate.py examples\n",
    "        output, duration = run_inference(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            prompt=prompt, \n",
    "            steps=128, \n",
    "            gen_length=128, \n",
    "            block_length=32\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"wall_time\": duration,\n",
    "            \"output_length\": len(output),\n",
    "            \"output\": output\n",
    "        })\n",
    "        \n",
    "\t\t# Add task_id to each step detail\n",
    "\t\tfor d in step_details:\n",
    "\t\t\td['task_id'] = task_id\n",
    "\t\tall_step_details.extend(step_details)\n",
    "\n",
    "\t\tprint(f\"  Time: {duration:.4f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\t\timport traceback\n",
    "\t\ttraceback.print_exc()\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_steps = pd.DataFrame(all_step_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Statistics\n",
    "if not df_results.empty:\n",
    "    stats = {\n",
    "        \"Mean Latency\": df_results['wall_time'].mean(),\n",
    "        \"Median Latency\": df_results['wall_time'].median(),\n",
    "        \"Std Dev\": df_results['wall_time'].std(),\n",
    "        \"Min\": df_results['wall_time'].min(),\n",
    "        \"Max\": df_results['wall_time'].max(),\n",
    "        \"P95\": df_results['wall_time'].quantile(0.95),\n",
    "        \"P99\": df_results['wall_time'].quantile(0.99)\n",
    "    }\n",
    "\n",
    "    print(\"Wall Time Statistics (seconds):\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"No results to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de144847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Latency\n",
    "if not df_results.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_results['wall_time'], kde=True, bins=10)\n",
    "    plt.title('Inference Wall Time Distribution')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(df_results['wall_time'].mean(), color='r', linestyle='--', label=f\"Mean: {df_results['wall_time'].mean():.2f}s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Display Step Statistics\n",
    "if not df_steps.empty:\n",
    "    print(\"\\nStep-level Statistics (First 5 steps):\")\n",
    "    print(df_steps.head())\n",
    "    \n",
    "    # Plot Step Times\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df_steps, x='step_idx', y='duration', hue='block_idx', marker='o')\n",
    "    plt.title('Inference Time per Step across Blocks')\n",
    "    plt.xlabel('Step Index')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Remasking\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.lineplot(data=df_steps, x='step_idx', y='remasked_count', hue='block_idx', marker='x')\n",
    "    plt.title('Tokens Remasked per Step')\n",
    "    plt.xlabel('Step Index')\n",
    "    plt.ylabel('Count of Remasked Tokens')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
