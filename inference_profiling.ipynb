{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4162f6e",
   "metadata": {},
   "source": [
    "# LLaDA Inference Profiling on HumanEval\n",
    "\n",
    "This notebook runs inference on the LLaDA model using the HumanEval dataset and collects statistics for profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bd180",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ead82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_HOME'] = '/root/LLaDA/hf_models/'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from generate import generate\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "cache_path = '/root/LLaDA/hf_models/hub'\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, cache_dir=cache_path, local_files_only=True).to(device).eval()\n",
    "\n",
    "# Ensure padding side is left for generation\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf80dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HumanEval Dataset\n",
    "print(\"Loading HumanEval dataset...\")\n",
    "dataset_path = '/root/LLaDA/hf_models/datasets/openai_humaneval'\n",
    "ds = load_dataset(path=dataset_path, split=\"test\")\n",
    "print(f\"Loaded {len(ds)} problems.\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample Problem:\")\n",
    "print(ds[0]['prompt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8e153",
   "metadata": {},
   "source": [
    "## Run Model Inference and Collect Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n",
    "\n",
    "\n",
    "class ActivationProfiler:\n",
    "    def __init__(self, model, target_layers=['k_proj', 'v_proj', 'o_proj'], save_dir='../autodl-tmp/profiling_results'):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.hooks = []\n",
    "        self.save_dir = save_dir\n",
    "        self.current_step = 0\n",
    "        self.buffer = {} # name -> tensor\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def register_hooks(self):\n",
    "        self.clear()\n",
    "        for name, module in self.model.named_modules():\n",
    "            # Check if the module name ends with any of the target layers\n",
    "            if any(name.endswith(t) for t in self.target_layers):\n",
    "                # We only want leaf modules, usually\n",
    "                hook = module.register_forward_hook(self.get_hook(name))\n",
    "                self.hooks.append(hook)\n",
    "        print(f\"Registered hooks on {len(self.hooks)} layers.\")\n",
    "\n",
    "    def get_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            # output shape: [batch, seq_len, hidden_dim]\n",
    "            # Buffer to CPU memory instead of saving immediately to avoid I/O crashes\n",
    "            self.buffer[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    def step(self, current_step_index):\n",
    "        \"\"\"Sets the current step index for saving.\"\"\"\n",
    "        self.current_step = current_step_index\n",
    "\n",
    "    def save_buffer(self):\n",
    "        \"\"\"Saves buffered tensors to disk sequentially.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "            \n",
    "        # Create a directory for the current step\n",
    "        step_dir = os.path.join(self.save_dir, f\"step_{self.current_step}\")\n",
    "        os.makedirs(step_dir, exist_ok=True)\n",
    "        \n",
    "        for name, tensor in self.buffer.items():\n",
    "            # Replace dots with underscores for safe filenames\n",
    "            safe_name = name.replace('.', '_')\n",
    "            file_path = os.path.join(step_dir, f\"{safe_name}.pt\")\n",
    "            torch.save(tensor, file_path)\n",
    "        \n",
    "        # Clear buffer to free memory\n",
    "        self.buffer = {}\n",
    "\n",
    "    def clear(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "        self.buffer = {}\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def get_collected_data(self):\n",
    "        return self.save_dir\n",
    "\n",
    "\n",
    "def save_token_state(save_dir, step_idx, prompt_mask, mask_index, transfer_index, block_idx, inblock_step):\n",
    "    \"\"\"Persist token-role metadata for later visualization.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    step_dir = os.path.join(save_dir, f\"step_{step_idx}\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "    payload = {\n",
    "        \"prompt_mask\": prompt_mask.to(dtype=torch.bool).cpu(),\n",
    "        \"mask_index\": mask_index.to(dtype=torch.bool).cpu(),\n",
    "        \"transfer_index\": transfer_index.to(dtype=torch.bool).cpu(),\n",
    "        \"block_idx\": block_idx,\n",
    "        \"inblock_step\": inblock_step,\n",
    "    }\n",
    "    torch.save(payload, os.path.join(step_dir, \"token_state.pt\"))\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def profiled_generate(model, prompt, attention_mask=None, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336, logits_eos_inf=False, confidence_eos_eot_inf=False, profiler=None):\n",
    "    '''\n",
    "    Revised generate function adhering to generate.py logic with profiling.\n",
    "    '''\n",
    "    # Initialize x with mask\n",
    "    x = torch.full((prompt.shape[0], prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((prompt.shape[0], gen_length), dtype=attention_mask.dtype, device=model.device)], dim=-1)\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    # Profiling data storage\n",
    "    step_details = []\n",
    "    \n",
    "    # Print inference statistics\n",
    "    print(f'Generation length: {gen_length}')\n",
    "    print(f'Denoising steps per block: {steps}')\n",
    "    print(f'Number of blocks: {num_blocks}')\n",
    "\n",
    "    # Helper for CUDA synchronization\n",
    "    def synchronize():\n",
    "        if x.device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    synchronize()\n",
    "    total_start = time.perf_counter()\n",
    "    for num_block in range(num_blocks):\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            global_step = num_block * steps + i\n",
    "            if profiler:\n",
    "                # print(f\"flag step: global={global_step} block={num_block} in_block={i}\")\n",
    "                profiler.step(global_step)\n",
    "            \n",
    "            synchronize()\n",
    "            step_start = time.perf_counter()\n",
    "            \n",
    "            mask_index = (x == mask_id)\n",
    "            \n",
    "            # 1. Model Forward Pass\n",
    "            t_forward_start = time.perf_counter()\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask_ = torch.cat([attention_mask, attention_mask], dim=0)\n",
    "                logits = model(x_, attention_mask=attention_mask_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x, attention_mask=attention_mask).logits\n",
    "            synchronize()\n",
    "            t_forward_end = time.perf_counter()\n",
    "\n",
    "            # Save buffered activations for this (global) step\n",
    "            if profiler:\n",
    "                profiler.save_buffer()\n",
    "\n",
    "            if logits_eos_inf:\n",
    "                logits[:, :, 126081] = -torch.inf\n",
    "\n",
    "            # 2. Sampling\n",
    "            t_sample_start = time.perf_counter()\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l\n",
    "            synchronize()\n",
    "            t_sample_end = time.perf_counter()\n",
    "\n",
    "            if confidence_eos_eot_inf:\n",
    "                logits_with_noise[:, :, 126081] = logits[:, :, 126348] = -torch.inf\n",
    "\n",
    "            # 3. Remasking Strategy\n",
    "            t_remask_start = time.perf_counter()\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "            if profiler is not None:\n",
    "                save_token_state(\n",
    "                    profiler.save_dir,\n",
    "                    global_step,\n",
    "                    prompt_index,\n",
    "                    mask_index,\n",
    "                    transfer_index,\n",
    "                    num_block,\n",
    "                    i,\n",
    "                )\n",
    "            \n",
    "            synchronize()\n",
    "            t_remask_end = time.perf_counter()\n",
    "            \n",
    "            step_end = time.perf_counter()\n",
    "            step_duration = step_end - step_start\n",
    "            \n",
    "            step_details.append({\n",
    "                \"block_idx\": num_block,\n",
    "                \"step_idx\": i,\n",
    "                \"duration\": step_duration,\n",
    "                \"forward_time\": t_forward_end - t_forward_start,\n",
    "                \"sampling_time\": t_sample_end - t_sample_start,\n",
    "                \"remasking_time\": t_remask_end - t_remask_start,\n",
    "                \"num_masks\": mask_index.sum().item()\n",
    "            })\n",
    "\n",
    "    synchronize()\n",
    "    total_end = time.perf_counter()\n",
    "    \n",
    "    return x, step_details\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, prompt_text, steps=64, gen_length=64, block_length=32, profiler=None):\n",
    "    \"\"\"\n",
    "    Runs inference and measures wall time with detailed profiling.\n",
    "    \"\"\"\n",
    "    # Prepare input\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out, step_details = profiled_generate(\n",
    "            model, \n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            steps=steps, \n",
    "            gen_length=gen_length, \n",
    "            block_length=block_length, \n",
    "            temperature=0., \n",
    "            cfg_scale=0., \n",
    "            remasking='low_confidence',\n",
    "            profiler=profiler\n",
    "        )\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text, wall_time, step_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e59ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on a Subset\n",
    "num_samples = 1\n",
    "results = []\n",
    "all_step_details = []\n",
    "\n",
    "# Initialize Profiler\n",
    "tensor_dir = '/root/autodl-tmp/profiling_results'\n",
    "profiler = ActivationProfiler(model, target_layers=['ff_out'], save_dir=tensor_dir)\n",
    "# profiler = ActivationProfiler(model, target_layers=['q_proj', 'k_proj', 'v_proj', 'attn_out'], save_dir=tensor_dir)\n",
    "\n",
    "# Ensure hooks are cleared initially\n",
    "profiler.clear()\n",
    "\n",
    "print(f\"Running inference on first {num_samples} samples...\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    problem = ds[i]\n",
    "    prompt = problem['prompt']\n",
    "    task_id = problem['task_id']\n",
    "    \n",
    "    print(f\"Processing {task_id}...\")\n",
    "\n",
    "\t# Register hooks for this run\n",
    "    profiler.register_hooks()\n",
    "\n",
    "    try:\n",
    "        # Using parameters from chat.py/generate.py examples\n",
    "        output, duration, step_details = run_inference(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            prompt_text=prompt, \n",
    "            steps=128, \n",
    "            gen_length=128, \n",
    "            block_length=32,\n",
    "            profiler=profiler\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"wall_time\": duration,\n",
    "            \"output_length\": len(output),\n",
    "            \"output\": output\n",
    "        })\n",
    "        \n",
    "\t\t# Add task_id to each step detail\n",
    "        for d in step_details:\n",
    "            d['task_id'] = task_id\n",
    "        all_step_details.extend(step_details)\n",
    "        print(f\"  Time: {duration:.4f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Remove hooks to free memory and prevent crashes in subsequent runs\n",
    "        profiler.clear()\n",
    "\n",
    "profiler.clear() # Remove hooks after done\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_steps = pd.DataFrame(all_step_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f300f",
   "metadata": {},
   "source": [
    "## Statistics Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93248f7d",
   "metadata": {},
   "source": [
    "### Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Wall-time stats\n",
    "    stats = {\n",
    "        \"Mean Latency\": df_results['wall_time'].mean(),\n",
    "        \"Median Latency\": df_results['wall_time'].median(),\n",
    "        \"Std Dev\": df_results['wall_time'].std(),\n",
    "        \"Min\": df_results['wall_time'].min(),\n",
    "        \"Max\": df_results['wall_time'].max(),\n",
    "        \"P95\": df_results['wall_time'].quantile(0.95),\n",
    "        \"P99\": df_results['wall_time'].quantile(0.99),\n",
    "    }\n",
    "    print(\"Wall Time Statistics (seconds):\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_results['wall_time'], kde=True, bins=10)\n",
    "    plt.title('Inference Wall Time Distribution')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(df_results['wall_time'].mean(), color='r', linestyle='--', label=f\"Mean: {df_results['wall_time'].mean():.2f}s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Per-step breakdown\n",
    "if not df_steps.empty:\n",
    "    # Totals\n",
    "    totals = df_steps[['forward_time', 'sampling_time', 'remasking_time']].sum()\n",
    "    total_time = totals.sum()\n",
    "    print(\"\\nBreakdown (sum over all steps):\")\n",
    "    for name, val in totals.items():\n",
    "        pct = 100.0 * val / total_time if total_time > 0 else 0.0\n",
    "        print(f\"  {name}: {val:.4f}s ({pct:.1f}%)\")\n",
    "\n",
    "    # Stacked bar per step\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    steps_idx = np.arange(len(df_steps))\n",
    "    fwd = df_steps['forward_time'].values\n",
    "    smp = df_steps['sampling_time'].values\n",
    "    rmk = df_steps['remasking_time'].values\n",
    "    plt.bar(steps_idx, fwd, label='forward')\n",
    "    plt.bar(steps_idx, smp, bottom=fwd, label='sampling')\n",
    "    plt.bar(steps_idx, rmk, bottom=fwd + smp, label='remasking')\n",
    "    plt.title('Per-step Latency Breakdown')\n",
    "    plt.xlabel('Global step')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: aggregate by block\n",
    "    if 'block_idx' in df_steps.columns:\n",
    "        block_agg = df_steps.groupby('block_idx')[['forward_time','sampling_time','remasking_time']].sum()\n",
    "        block_agg.plot(kind='bar', stacked=True, figsize=(8,4), title='Latency Breakdown per Block')\n",
    "        plt.xlabel('Block index')\n",
    "        plt.ylabel('Time (s)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf87b0",
   "metadata": {},
   "source": [
    "### Token State Evolution\n",
    "\n",
    "This section analyzes how token states evolve during the denoising process by examining the saved token state data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e28b13",
   "metadata": {},
   "source": [
    "#### Token State Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Token state analysis utilities\n",
    "# =========================\n",
    "\n",
    "\n",
    "def load_token_state(base_dir, step):\n",
    "    \"\"\"Load token state data for a specific step.\"\"\"\n",
    "    step_dir = os.path.join(base_dir, f\"step_{step}\")\n",
    "    token_state_path = os.path.join(step_dir, \"token_state.pt\")\n",
    "    if os.path.exists(token_state_path):\n",
    "        return torch.load(token_state_path, map_location=\"cpu\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def collect_token_evolution(base_dir, steps=None):\n",
    "    \"\"\"\n",
    "    Collect token state evolution across all steps.\n",
    "    Returns DataFrame with columns: step, token_idx, state, block_idx, inblock_step\n",
    "    \"\"\"\n",
    "    if steps is None:\n",
    "        step_dirs = sorted([d for d in os.listdir(base_dir) if d.startswith(\"step_\")],\n",
    "                          key=lambda x: int(x.split('_')[1]))\n",
    "        steps = [int(d.split('_')[1]) for d in step_dirs]\n",
    "    \n",
    "    rows = []\n",
    "    for step in steps:\n",
    "        token_state = load_token_state(base_dir, step)\n",
    "        if token_state is None:\n",
    "            continue\n",
    "            \n",
    "        prompt_mask = token_state[\"prompt_mask\"][0]  # [seq_len]\n",
    "        mask_index = token_state[\"mask_index\"][0]    # [seq_len]\n",
    "        transfer_index = token_state[\"transfer_index\"][0]  # [seq_len]\n",
    "        block_idx = token_state[\"block_idx\"]\n",
    "        inblock_step = token_state[\"inblock_step\"]\n",
    "        \n",
    "        seq_len = prompt_mask.shape[0]\n",
    "        for token_idx in range(seq_len):\n",
    "            # Determine token state\n",
    "            if prompt_mask[token_idx]:\n",
    "                state = \"prompt\"\n",
    "            elif mask_index[token_idx]:\n",
    "                if transfer_index[token_idx]:\n",
    "                    state = \"transferred\"\n",
    "                else:\n",
    "                    state = \"masked\"\n",
    "            else:\n",
    "                state = \"generated\"\n",
    "            \n",
    "            rows.append({\n",
    "                \"step\": step,\n",
    "                \"token_idx\": token_idx,\n",
    "                \"state\": state,\n",
    "                \"block_idx\": block_idx,\n",
    "                \"inblock_step\": inblock_step,\n",
    "                \"is_prompt\": prompt_mask[token_idx].item(),\n",
    "                \"is_masked\": mask_index[token_idx].item(),\n",
    "                \"is_transferred\": transfer_index[token_idx].item()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def analyze_token_transitions(df_tokens):\n",
    "    \"\"\"Analyze how tokens transition between states.\"\"\"\n",
    "    transitions = []\n",
    "    \n",
    "    # Group by token and track state changes\n",
    "    for token_idx in df_tokens['token_idx'].unique():\n",
    "        token_history = df_tokens[df_tokens['token_idx'] == token_idx].sort_values('step')\n",
    "        \n",
    "        prev_state = None\n",
    "        for _, row in token_history.iterrows():\n",
    "            current_state = row['state']\n",
    "            if prev_state is not None and prev_state != current_state:\n",
    "                transitions.append({\n",
    "                    'token_idx': token_idx,\n",
    "                    'step': row['step'],\n",
    "                    'from_state': prev_state,\n",
    "                    'to_state': current_state,\n",
    "                    'block_idx': row['block_idx']\n",
    "                })\n",
    "            prev_state = current_state\n",
    "    \n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "\n",
    "def get_token_coverage_stats(df_tokens):\n",
    "    \"\"\"Get statistics about token coverage at each step.\"\"\"\n",
    "    coverage_stats = []\n",
    "    \n",
    "    for step in sorted(df_tokens['step'].unique()):\n",
    "        step_data = df_tokens[df_tokens['step'] == step]\n",
    "        \n",
    "        total_tokens = len(step_data)\n",
    "        prompt_tokens = len(step_data[step_data['state'] == 'prompt'])\n",
    "        masked_tokens = len(step_data[step_data['state'] == 'masked'])\n",
    "        generated_tokens = len(step_data[step_data['state'] == 'generated'])\n",
    "        transferred_tokens = len(step_data[step_data['state'] == 'transferred'])\n",
    "        \n",
    "        coverage_stats.append({\n",
    "            'step': step,\n",
    "            'total_tokens': total_tokens,\n",
    "            'prompt_tokens': prompt_tokens,\n",
    "            'masked_tokens': masked_tokens,\n",
    "            'generated_tokens': generated_tokens,\n",
    "            'transferred_tokens': transferred_tokens,\n",
    "            'mask_ratio': masked_tokens / total_tokens if total_tokens > 0 else 0,\n",
    "            'generated_ratio': generated_tokens / total_tokens if total_tokens > 0 else 0,\n",
    "            'block_idx': step_data['block_idx'].iloc[0] if len(step_data) > 0 else None,\n",
    "            'inblock_step': step_data['inblock_step'].iloc[0] if len(step_data) > 0 else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(coverage_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048a2bb",
   "metadata": {},
   "source": [
    "#### Token Evolution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect token evolution data\n",
    "print(\"Analyzing token state evolution...\")\n",
    "df_token_evolution = collect_token_evolution(tensor_dir)\n",
    "\n",
    "if not df_token_evolution.empty:\n",
    "    print(f\"Loaded token states for {len(df_token_evolution['step'].unique())} steps\")\n",
    "    print(f\"Sequence length: {df_token_evolution['token_idx'].max() + 1}\")\n",
    "    \n",
    "    # Get coverage statistics\n",
    "    df_coverage = get_token_coverage_stats(df_token_evolution)\n",
    "    \n",
    "    # 1. Token state heatmap over time\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create a matrix for visualization: steps x tokens\n",
    "    steps = sorted(df_token_evolution['step'].unique())\n",
    "    tokens = sorted(df_token_evolution['token_idx'].unique())\n",
    "    \n",
    "    # Map states to numbers for visualization\n",
    "    state_map = {'prompt': 0, 'masked': 1, 'transferred': 2, 'generated': 3}\n",
    "    state_matrix = np.full((len(steps), len(tokens)), -1)\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        step_data = df_token_evolution[df_token_evolution['step'] == step]\n",
    "        for _, row in step_data.iterrows():\n",
    "            j = row['token_idx']\n",
    "            state_matrix[i, j] = state_map[row['state']]\n",
    "    \n",
    "    # Create custom colormap\n",
    "    colors = ['blue', 'red', 'orange', 'green']  # prompt, masked, transferred, generated\n",
    "    cmap = plt.matplotlib.colors.ListedColormap(colors)\n",
    "    \n",
    "    plt.imshow(state_matrix, cmap=cmap, aspect='auto', interpolation='nearest')\n",
    "    plt.xlabel('Token Index')\n",
    "    plt.ylabel('Denoising Step')\n",
    "    plt.title('Token State Evolution During Denoising')\n",
    "    \n",
    "    # Add colorbar with labels\n",
    "    cbar = plt.colorbar(ticks=[0, 1, 2, 3])\n",
    "    cbar.set_ticklabels(['Prompt', 'Masked', 'Transferred', 'Generated'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Token coverage evolution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(df_coverage['step'], df_coverage['mask_ratio'], 'r-', label='Masked Ratio')\n",
    "    plt.plot(df_coverage['step'], df_coverage['generated_ratio'], 'g-', label='Generated Ratio')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.title('Token Coverage Evolution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(df_coverage['step'], df_coverage['masked_tokens'], 'r-', label='Masked')\n",
    "    plt.plot(df_coverage['step'], df_coverage['generated_tokens'], 'g-', label='Generated')\n",
    "    plt.plot(df_coverage['step'], df_coverage['transferred_tokens'], 'orange', label='Transferred')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Token Count')\n",
    "    plt.title('Token Count by State')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Block-wise analysis\n",
    "    plt.subplot(2, 2, 3)\n",
    "    block_colors = plt.cm.Set3(np.linspace(0, 1, df_coverage['block_idx'].max() + 1))\n",
    "    for block_idx in sorted(df_coverage['block_idx'].unique()):\n",
    "        block_data = df_coverage[df_coverage['block_idx'] == block_idx]\n",
    "        plt.plot(block_data['inblock_step'], block_data['mask_ratio'], \n",
    "                color=block_colors[block_idx], label=f'Block {block_idx}', marker='o')\n",
    "    plt.xlabel('In-Block Step')\n",
    "    plt.ylabel('Mask Ratio')\n",
    "    plt.title('Mask Ratio by Block')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Transfer rate analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    transfer_rates = []\n",
    "    for step in steps[1:]:  # Skip first step\n",
    "        prev_step_data = df_token_evolution[df_token_evolution['step'] == step - 1]\n",
    "        curr_step_data = df_token_evolution[df_token_evolution['step'] == step]\n",
    "        \n",
    "        prev_masked = len(prev_step_data[prev_step_data['state'] == 'masked'])\n",
    "        curr_transferred = len(curr_step_data[curr_step_data['state'] == 'transferred'])\n",
    "        \n",
    "        transfer_rate = curr_transferred / prev_masked if prev_masked > 0 else 0\n",
    "        transfer_rates.append(transfer_rate)\n",
    "    \n",
    "    plt.plot(steps[1:], transfer_rates, 'b-o')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Transfer Rate')\n",
    "    plt.title('Token Transfer Rate per Step')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nToken Evolution Summary:\")\n",
    "    print(f\"Total denoising steps: {len(steps)}\")\n",
    "    print(f\"Initial masked tokens: {df_coverage.iloc[0]['masked_tokens']}\")\n",
    "    print(f\"Final generated tokens: {df_coverage.iloc[-1]['generated_tokens']}\")\n",
    "    print(f\"Average tokens transferred per step: {df_coverage['transferred_tokens'].mean():.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No token state data found. Make sure profiling was run with token state saving enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed4859",
   "metadata": {},
   "source": [
    "#### Detailed Token Transition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_token_evolution.empty:\n",
    "    # Analyze token transitions\n",
    "    df_transitions = analyze_token_transitions(df_token_evolution)\n",
    "    \n",
    "    if not df_transitions.empty:\n",
    "        print(f\"\\nFound {len(df_transitions)} token state transitions\")\n",
    "        \n",
    "        # Transition frequency analysis\n",
    "        transition_counts = df_transitions.groupby(['from_state', 'to_state']).size().reset_index(name='count')\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create transition matrix for heatmap\n",
    "        states = ['prompt', 'masked', 'transferred', 'generated']\n",
    "        transition_matrix = pd.pivot_table(transition_counts, \n",
    "                                         index='from_state', \n",
    "                                         columns='to_state', \n",
    "                                         values='count', \n",
    "                                         fill_value=0)\n",
    "        \n",
    "        # Reindex to ensure all states are included\n",
    "        transition_matrix = transition_matrix.reindex(index=states, columns=states, fill_value=0)\n",
    "        \n",
    "        sns.heatmap(transition_matrix, annot=True, cmap='Blues')\n",
    "        plt.title('Token State Transition Matrix')\n",
    "        plt.xlabel('To State')\n",
    "        plt.ylabel('From State')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Transition timing analysis\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        for transition_type in transition_counts[['from_state', 'to_state']].apply(\n",
    "            lambda x: f\"{x['from_state']} → {x['to_state']}\", axis=1).unique():\n",
    "            \n",
    "            from_state, to_state = transition_type.split(' → ')\n",
    "            transition_data = df_transitions[\n",
    "                (df_transitions['from_state'] == from_state) & \n",
    "                (df_transitions['to_state'] == to_state)\n",
    "            ]\n",
    "            \n",
    "            if len(transition_data) > 0:\n",
    "                step_counts = transition_data['step'].value_counts().sort_index()\n",
    "                plt.plot(step_counts.index, step_counts.values, \n",
    "                        marker='o', label=transition_type, alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Number of Transitions')\n",
    "        plt.title('Token State Transitions Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTransition Summary:\")\n",
    "        print(transition_counts.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No state transitions detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c7c6d",
   "metadata": {},
   "source": [
    "#### Token-Level Detailed Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_token_evolution.empty:\n",
    "    # Analyze specific token ranges\n",
    "    seq_len = df_token_evolution['token_idx'].max() + 1\n",
    "    prompt_len = df_token_evolution[df_token_evolution['state'] == 'prompt']['token_idx'].max() + 1\n",
    "    gen_len = seq_len - prompt_len\n",
    "    \n",
    "    print(f\"\\nSequence Analysis:\")\n",
    "    print(f\"Total sequence length: {seq_len}\")\n",
    "    print(f\"Prompt length: {prompt_len}\")\n",
    "    print(f\"Generation length: {gen_len}\")\n",
    "    \n",
    "    # Focus on generation region\n",
    "    gen_tokens = df_token_evolution[df_token_evolution['token_idx'] >= prompt_len].copy()\n",
    "    \n",
    "    if not gen_tokens.empty:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Generation region heatmap\n",
    "        gen_steps = sorted(gen_tokens['step'].unique())\n",
    "        gen_token_indices = sorted(gen_tokens['token_idx'].unique())\n",
    "        \n",
    "        state_matrix_gen = np.full((len(gen_steps), len(gen_token_indices)), -1)\n",
    "        \n",
    "        for i, step in enumerate(gen_steps):\n",
    "            step_data = gen_tokens[gen_tokens['step'] == step]\n",
    "            for _, row in step_data.iterrows():\n",
    "                j = gen_token_indices.index(row['token_idx'])\n",
    "                state_matrix_gen[i, j] = state_map[row['state']]\n",
    "        \n",
    "        plt.imshow(state_matrix_gen, cmap=cmap, aspect='auto', interpolation='nearest')\n",
    "        plt.xlabel('Generation Token Index')\n",
    "        plt.ylabel('Denoising Step')\n",
    "        plt.title('Token State Evolution in Generation Region')\n",
    "        \n",
    "        # Adjust x-axis labels to show actual token indices\n",
    "        n_ticks = min(10, len(gen_token_indices))\n",
    "        tick_indices = np.linspace(0, len(gen_token_indices)-1, n_ticks, dtype=int)\n",
    "        plt.xticks(tick_indices, [gen_token_indices[i] for i in tick_indices])\n",
    "        \n",
    "        cbar = plt.colorbar(ticks=[0, 1, 2, 3])\n",
    "        cbar.set_ticklabels(['Prompt', 'Masked', 'Transferred', 'Generated'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Block boundary analysis\n",
    "        block_transitions = gen_tokens.groupby(['step', 'block_idx']).size().reset_index(name='count')\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for block_idx in sorted(block_transitions['block_idx'].unique()):\n",
    "            block_data = df_coverage[df_coverage['block_idx'] == block_idx]\n",
    "            plt.axvspan(block_data['step'].min(), block_data['step'].max(), \n",
    "                       alpha=0.3, label=f'Block {block_idx}')\n",
    "        \n",
    "        plt.plot(df_coverage['step'], df_coverage['generated_tokens'], 'g-', linewidth=2)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Generated Tokens')\n",
    "        plt.title('Token Generation Progress with Block Boundaries')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Token state evolution analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5a365",
   "metadata": {},
   "source": [
    "### Tensor Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e0d50",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d665a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Tensor loader utilities (saved to disk during inference)\n",
    "# =========================\n",
    "import glob, re\n",
    "\n",
    "_layer_re = re.compile(\n",
    "    r\".*_(?:layers|transformer_blocks)_(\\d+).*_(q_proj|k_proj|v_proj|attn_out|ff_proj|up_proj|ff_out)\\.pt$\"\n",
    ")\n",
    "tensor_dir = '/root/autodl-tmp/profiling_results'\n",
    "\n",
    "\n",
    "def load_step_tensors(base_dir, step, selector_substrings):\n",
    "    \"\"\"\n",
    "    Returns {layer_file: tensor} for files containing any selector substring.\n",
    "    selector_substrings: list[str], e.g., ['q_proj', 'k_proj'].\n",
    "    \"\"\"\n",
    "    step_dir = os.path.join(base_dir, f\"step_{step}\")\n",
    "    tensors = {}\n",
    "    if not os.path.isdir(step_dir):\n",
    "        return tensors\n",
    "    for f in glob.glob(os.path.join(step_dir, \"*.pt\")):\n",
    "        base = os.path.basename(f)\n",
    "        if any(s in base for s in selector_substrings):\n",
    "            tensors[base] = torch.load(f, map_location=\"cpu\")\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def parse_layer_and_kind(layer_file_basename):\n",
    "    m = _layer_re.match(layer_file_basename)\n",
    "    if m:\n",
    "        return int(m.group(1)), m.group(2)\n",
    "    for k in ['q_proj','k_proj','v_proj','attn_out','ff_proj','up_proj','ff_out']:\n",
    "        if k in layer_file_basename:\n",
    "            return None, k\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def reduce_stat(t, how=\"mean_abs\", sample=None):\n",
    "    \"\"\"\n",
    "    Reduce a tensor [batch, seq, hidden] to a scalar.\n",
    "    how: 'mean_abs' | 'max_abs' | 'std' | 'l2'\n",
    "    sample: dict to subsample dims, e.g., {'seq': 256, 'hidden': 512}\n",
    "    \"\"\"\n",
    "    x = t\n",
    "    # optional subsample to reduce memory/plot time\n",
    "    if sample:\n",
    "        b, s, h = x.shape\n",
    "        if 'seq' in sample:\n",
    "            x = x[:, torch.linspace(0, s-1, steps=min(sample['seq'], s), dtype=torch.long), :]\n",
    "        if 'hidden' in sample:\n",
    "            x = x[:, :, torch.linspace(0, h-1, steps=min(sample['hidden'], h), dtype=torch.long)]\n",
    "    x = x.float()\n",
    "    if how == \"mean_abs\":\n",
    "        return x.abs().mean().item()\n",
    "    if how == \"max_abs\":\n",
    "        return x.abs().amax().item()\n",
    "    if how == \"std\":\n",
    "        return x.std().item()\n",
    "    if how == \"l2\":\n",
    "        return x.pow(2).mean().sqrt().item()\n",
    "    raise ValueError(how)\n",
    "\n",
    "\n",
    "def collect_series(base_dir, selectors=('q_proj','k_proj'), how=\"mean_abs\", sample=None, steps=None):\n",
    "    \"\"\"\n",
    "    Scan steps->layers and compute a scalar stat per (step, layer, kind).\n",
    "    Returns DataFrame columns: step, layer_idx, kind, value\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # infer steps if not provided\n",
    "    if steps is None:\n",
    "        step_dirs = sorted([d for d in os.listdir(base_dir) if d.startswith(\"step_\")],\n",
    "                           key=lambda x: int(x.split('_')[1]))\n",
    "        steps = [int(d.split('_')[1]) for d in step_dirs]\n",
    "    for step in steps:\n",
    "        tensors = load_step_tensors(base_dir, step, selectors)\n",
    "        for name, t in tensors.items():\n",
    "            layer_idx, kind = parse_layer_and_kind(name)\n",
    "            if kind is None: \n",
    "                continue\n",
    "            val = reduce_stat(t, how=how, sample=sample)\n",
    "            rows.append({\"step\": step, \"layer_idx\": layer_idx, \"kind\": kind, \"value\": val, \"name\": name})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def flatten_activation(t, mode=\"batch_seq_mean\"):\n",
    "    x = t.float()\n",
    "    if mode == \"batch_seq_mean\":\n",
    "        return x.mean(dim=(0, 1))                        # [hidden]\n",
    "    if mode == \"batch_mean\":\n",
    "        return x.mean(dim=0).reshape(-1, x.size(-1))     # [seq, hidden]\n",
    "    if mode == \"none\":\n",
    "        return x.reshape(-1, x.size(-1))                 # [tokens, hidden]\n",
    "    raise ValueError(mode)\n",
    "\n",
    "\n",
    "def _token_matrix(x):\n",
    "    x = x.float()\n",
    "    if x.ndim == 2:\n",
    "        return x\n",
    "    if x.ndim == 3:\n",
    "        return x.view(-1, x.size(-1))\n",
    "    raise ValueError(f\"Unsupported tensor shape {tuple(x.shape)} for token-wise mode\")\n",
    "\n",
    "\n",
    "def cosine_similarity_tensors(t_a, t_b, mode=\"batch_seq_mean\"):\n",
    "    if mode == \"token-wise\":\n",
    "        a = _token_matrix(t_a)\n",
    "        b = _token_matrix(t_b)\n",
    "        sims = F.cosine_similarity(a, b, dim=-1)\n",
    "        return sims.cpu().numpy()\n",
    "    elif mode == 'batch_seq_mean':\n",
    "        a = flatten_activation(t_a, 'batch_seq_mean')\n",
    "        b = flatten_activation(t_b, 'batch_seq_mean')\n",
    "        if a.ndim == 1:\n",
    "            return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "        sims = F.cosine_similarity(a, b, dim=-1)\n",
    "        return sims.mean().item()\n",
    "    elif mode == 'batch_mean':\n",
    "        a = flatten_activation(t_a, 'batch_mean')          # [seq, hidden]\n",
    "        b = flatten_activation(t_b, 'batch_mean')\n",
    "        sims = F.cosine_similarity(a, b, dim=-1)\n",
    "        return sims.mean().item()\n",
    "    else:\n",
    "        raise ValueError(mode)\n",
    "\n",
    "\n",
    "def expand_tokenwise_similarity(df_sim):\n",
    "    rows = []\n",
    "    for _, row in df_sim.iterrows():\n",
    "        sims = row[\"similarity\"]\n",
    "        if isinstance(sims, np.ndarray) and row[\"layer_idx\"] is not None:\n",
    "            for token_idx, val in enumerate(sims):\n",
    "                rows.append({\n",
    "                    \"step_pair\": f\"{row['step_a']}→{row['step_b']}\",\n",
    "                    \"layer_idx\": row[\"layer_idx\"],\n",
    "                    \"name\": row[\"name\"],\n",
    "                    \"token_idx\": token_idx,\n",
    "                    \"token_similarity\": float(val),\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def collect_similarity_for_pairs(base_dir, step_pairs, selector=\"attn_out\", tensor_names=None, mode=\"batch_seq_mean\"):\n",
    "    rows = []\n",
    "    for step_a, step_b in step_pairs:\n",
    "        tensors_a = load_step_tensors(base_dir, step_a, [selector])\n",
    "        tensors_b = load_step_tensors(base_dir, step_b, [selector])\n",
    "        candidates = tensors_a.keys() if not tensor_names else [\n",
    "            name for name in tensors_a if any(tag in name for tag in tensor_names)\n",
    "        ]\n",
    "        for name in candidates:\n",
    "            if name not in tensors_b:\n",
    "                continue\n",
    "            layer_idx, kind = parse_layer_and_kind(name)\n",
    "            sim = cosine_similarity_tensors(tensors_a[name], tensors_b[name], mode=mode)\n",
    "            rows.append({\n",
    "                \"step_a\": step_a,\n",
    "                \"step_b\": step_b,\n",
    "                \"layer_idx\": layer_idx,\n",
    "                \"kind\": kind,\n",
    "                \"name\": name,\n",
    "                \"similarity\": sim,\n",
    "            })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465f80b",
   "metadata": {},
   "source": [
    "#### Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217dbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of mean_abs over (steps x layers) for q_proj\n",
    "df_q = collect_series(tensor_dir, selectors=('q_proj',), how=\"mean_abs\", sample={\"seq\":256, \"hidden\":512})\n",
    "if not df_q.empty and df_q['layer_idx'].notna().all():\n",
    "    pivot = df_q.pivot_table(index=\"step\", columns=\"layer_idx\", values=\"value\", aggfunc=\"mean\").sort_index()\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.heatmap(pivot.T, cmap=\"viridis\", cbar_kws={\"label\": \"mean|x|\"}, robust=True)\n",
    "    plt.title(\"q_proj mean_abs across layers (cols) and steps (rows)\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Layer index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99232a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare q_proj vs k_proj averaged over layers per step\n",
    "df_qk = collect_series(tensor_dir, selectors=('q_proj','k_proj'), how=\"max_abs\", sample={\"seq\":256, \"hidden\":512})\n",
    "if not df_qk.empty:\n",
    "    df_avg = df_qk.groupby(['step','kind'])['value'].mean().reset_index()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.lineplot(data=df_avg, x='step', y='value', hue='kind')\n",
    "    plt.title(\"q_proj vs k_proj (avg over layers) — max_abs\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"max|x|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a208673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution at a given step for specific layers/kind\n",
    "TARGET_STEP = df_q['step'].max() if not df_q.empty else None\n",
    "if TARGET_STEP is not None:\n",
    "    tensors = load_step_tensors(tensor_dir, TARGET_STEP, ['q_proj'])\n",
    "    # pick first layer entry for demo\n",
    "    if tensors:\n",
    "        name, tensor = sorted(tensors.items())[0]\n",
    "        x = tensor.float().view(-1).numpy()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.histplot(x, bins=100, kde=True)\n",
    "        plt.title(f\"Distribution of {name} at step={TARGET_STEP}\")\n",
    "        plt.xlabel(\"Activation value\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fab34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced cosine similarity analysis with comprehensive token state tracking\n",
    "def enhanced_cosine_similarity_with_full_evolution():\n",
    "    \"\"\"\n",
    "    Analyze cosine similarity with complete token state evolution tracking.\n",
    "    First loads all token states to understand the full evolution.\n",
    "    \"\"\"\n",
    "    STEP_PAIRS = [(0, 1), (31, 32), (63, 64), (126, 127)]\n",
    "    TENSOR_NAMES = None\n",
    "    MODE = \"token-wise\"\n",
    "    \n",
    "    # First, load ALL available token states to track evolution\n",
    "    def load_all_token_states(base_dir):\n",
    "        \"\"\"Load token states for all available steps.\"\"\"\n",
    "        token_states = {}\n",
    "        step_dirs = sorted([d for d in os.listdir(base_dir) if d.startswith(\"step_\")],\n",
    "                          key=lambda x: int(x.split('_')[1]))\n",
    "        \n",
    "        for step_dir in step_dirs:\n",
    "            step_num = int(step_dir.split('_')[1])\n",
    "            token_state_path = os.path.join(base_dir, step_dir, \"token_state.pt\")\n",
    "            if os.path.exists(token_state_path):\n",
    "                token_states[step_num] = torch.load(token_state_path, map_location=\"cpu\")\n",
    "        \n",
    "        return token_states\n",
    "    \n",
    "    def build_comprehensive_token_evolution(all_token_states):\n",
    "        \"\"\"\n",
    "        Build comprehensive token evolution by tracking each token across all steps.\n",
    "        \"\"\"\n",
    "        if not all_token_states:\n",
    "            return None\n",
    "            \n",
    "        # Get the sequence length from the first available state\n",
    "        first_step = min(all_token_states.keys())\n",
    "        seq_len = len(all_token_states[first_step][\"prompt_mask\"][0])\n",
    "        \n",
    "        # Initialize token evolution tracking\n",
    "        token_evolution = {}  # token_idx -> list of states across steps\n",
    "        \n",
    "        for token_idx in range(seq_len):\n",
    "            token_evolution[token_idx] = []\n",
    "        \n",
    "        # Track evolution across all steps\n",
    "        sorted_steps = sorted(all_token_states.keys())\n",
    "        \n",
    "        for step in sorted_steps:\n",
    "            state = all_token_states[step]\n",
    "            prompt_mask = state[\"prompt_mask\"][0]\n",
    "            mask_index = state[\"mask_index\"][0] \n",
    "            transfer_index = state[\"transfer_index\"][0]\n",
    "            \n",
    "            for token_idx in range(seq_len):\n",
    "                if prompt_mask[token_idx]:\n",
    "                    token_state = \"prompt\"\n",
    "                elif mask_index[token_idx]:\n",
    "                    if transfer_index[token_idx]:\n",
    "                        token_state = \"transferred\"\n",
    "                    else:\n",
    "                        token_state = \"masked\"\n",
    "                else:\n",
    "                    # Not prompt, not currently masked = generated\n",
    "                    token_state = \"generated\"\n",
    "                \n",
    "                token_evolution[token_idx].append({\n",
    "                    'step': step,\n",
    "                    'state': token_state,\n",
    "                    'block_idx': state[\"block_idx\"],\n",
    "                    'inblock_step': state[\"inblock_step\"]\n",
    "                })\n",
    "        \n",
    "        return token_evolution, sorted_steps\n",
    "    \n",
    "    def get_token_state_at_step(token_evolution, step):\n",
    "        \"\"\"Get token states for all tokens at a specific step.\"\"\"\n",
    "        token_states = []\n",
    "        for token_idx in range(len(token_evolution)):\n",
    "            # Find the state at this step\n",
    "            for state_info in token_evolution[token_idx]:\n",
    "                if state_info['step'] == step:\n",
    "                    token_states.append(state_info['state'])\n",
    "                    break\n",
    "            else:\n",
    "                # If step not found, assume it's the last known state\n",
    "                if token_evolution[token_idx]:\n",
    "                    token_states.append(token_evolution[token_idx][-1]['state'])\n",
    "                else:\n",
    "                    token_states.append(\"unknown\")\n",
    "        return token_states\n",
    "    \n",
    "    # Load all token states\n",
    "    print(\"Loading all token states for comprehensive evolution tracking...\")\n",
    "    all_token_states = load_all_token_states(tensor_dir)\n",
    "    \n",
    "    if not all_token_states:\n",
    "        print(\"No token state data found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded token states for {len(all_token_states)} steps: {sorted(all_token_states.keys())}\")\n",
    "    \n",
    "    # Build comprehensive evolution\n",
    "    token_evolution, all_steps = build_comprehensive_token_evolution(all_token_states)\n",
    "    \n",
    "    if token_evolution is None:\n",
    "        print(\"Could not build token evolution.\")\n",
    "        return\n",
    "    \n",
    "    # Get sequence information\n",
    "    first_step = min(all_token_states.keys())\n",
    "    prompt_len = all_token_states[first_step][\"prompt_mask\"][0].sum().item()\n",
    "    seq_len = len(token_evolution)\n",
    "    gen_len = seq_len - prompt_len\n",
    "    block_length = 32  # From your configuration\n",
    "    \n",
    "    # Calculate block boundaries\n",
    "    block_boundaries = []\n",
    "    for i in range(0, gen_len, block_length):\n",
    "        block_boundaries.append(prompt_len + i)\n",
    "    if prompt_len + gen_len not in block_boundaries:\n",
    "        block_boundaries.append(prompt_len + gen_len)\n",
    "    \n",
    "    print(f\"Sequence info: Total={seq_len}, Prompt={prompt_len}, Generation={gen_len}\")\n",
    "    print(f\"Block boundaries: {block_boundaries}\")\n",
    "    \n",
    "    # Collect similarity data\n",
    "    df_sim = collect_similarity_for_pairs(\n",
    "        tensor_dir,\n",
    "        STEP_PAIRS,\n",
    "        selector=\"attn_out\",\n",
    "        tensor_names=TENSOR_NAMES,\n",
    "        mode=MODE,\n",
    "    )\n",
    "    \n",
    "    if df_sim.empty:\n",
    "        print(\"No overlapping tensors for the requested pairs.\")\n",
    "        return\n",
    "    \n",
    "    if MODE == \"token-wise\":\n",
    "        df_tok = expand_tokenwise_similarity(df_sim)\n",
    "        if df_tok.empty:\n",
    "            print(\"Token-wise similarities lack valid layer indices to plot.\")\n",
    "            return\n",
    "            \n",
    "        for step_pair, group in df_tok.groupby(\"step_pair\"):\n",
    "            step_a, step_b = map(int, step_pair.split('→'))\n",
    "            \n",
    "            # Get token states using comprehensive evolution\n",
    "            token_states_a = get_token_state_at_step(token_evolution, step_a)\n",
    "            token_states_b = get_token_state_at_step(token_evolution, step_b)\n",
    "            \n",
    "            # Get block info\n",
    "            block_idx_a = all_token_states[step_a][\"block_idx\"]\n",
    "            block_idx_b = all_token_states[step_b][\"block_idx\"]\n",
    "            \n",
    "            # Create the similarity pivot table\n",
    "            pivot = (\n",
    "                group.pivot_table(index=\"layer_idx\", columns=\"token_idx\", values=\"token_similarity\", aggfunc=\"mean\")\n",
    "                .sort_index()\n",
    "            )\n",
    "            \n",
    "            # Create enhanced visualization\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "            \n",
    "            # Main heatmap\n",
    "            im = ax1.imshow(\n",
    "                pivot.values,\n",
    "                cmap=\"coolwarm\",\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "                aspect='auto',\n",
    "                interpolation='nearest'\n",
    "            )\n",
    "            \n",
    "            # Add block boundary lines\n",
    "            for boundary in block_boundaries:\n",
    "                if boundary < seq_len:\n",
    "                    ax1.axvline(x=boundary-0.5, color='black', linestyle='-', linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Add prompt boundary\n",
    "            ax1.axvline(x=prompt_len-0.5, color='purple', linestyle='-', linewidth=3, alpha=0.8)\n",
    "            \n",
    "            ax1.set_title(f\"{step_pair}: Token-wise Cosine Similarity (attn_out)\\nBlack=Block boundaries, Purple=Prompt boundary\")\n",
    "            ax1.set_xlabel(\"Token Index\")\n",
    "            ax1.set_ylabel(\"Layer Index\")\n",
    "            \n",
    "            # Set y-axis ticks to show layer indices\n",
    "            if len(pivot.index) <= 20:\n",
    "                ax1.set_yticks(range(len(pivot.index)))\n",
    "                ax1.set_yticklabels(pivot.index)\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax1, label=\"Cosine Similarity\")\n",
    "            \n",
    "            # Token state visualization for step A\n",
    "            state_colors = {'prompt': 0, 'masked': 1, 'transferred': 2, 'generated': 3, 'unknown': 4}\n",
    "            state_array_a = [state_colors.get(state, 4) for state in token_states_a]\n",
    "            \n",
    "            ax2.imshow([state_array_a], cmap=plt.cm.Set3, aspect='auto', interpolation='nearest')\n",
    "            ax2.set_title(f\"Token States at Step {step_a} (Block {block_idx_a})\")\n",
    "            ax2.set_xlabel(\"Token Index\")\n",
    "            ax2.set_yticks([])\n",
    "            \n",
    "            # Add block boundaries\n",
    "            for boundary in block_boundaries:\n",
    "                if boundary < seq_len:\n",
    "                    ax2.axvline(x=boundary-0.5, color='black', linestyle='-', linewidth=2, alpha=0.7)\n",
    "            ax2.axvline(x=prompt_len-0.5, color='purple', linestyle='-', linewidth=3, alpha=0.8)\n",
    "            \n",
    "            # Token state visualization for step B\n",
    "            state_array_b = [state_colors.get(state, 4) for state in token_states_b]\n",
    "            \n",
    "            ax3.imshow([state_array_b], cmap=plt.cm.Set3, aspect='auto', interpolation='nearest')\n",
    "            ax3.set_title(f\"Token States at Step {step_b} (Block {block_idx_b})\")\n",
    "            ax3.set_xlabel(\"Token Index\")\n",
    "            ax3.set_yticks([])\n",
    "            \n",
    "            # Add block boundaries\n",
    "            for boundary in block_boundaries:\n",
    "                if boundary < seq_len:\n",
    "                    ax3.axvline(x=boundary-0.5, color='black', linestyle='-', linewidth=2, alpha=0.7)\n",
    "            ax3.axvline(x=prompt_len-0.5, color='purple', linestyle='-', linewidth=3, alpha=0.8)\n",
    "            \n",
    "            # Add legend for token states\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [\n",
    "                Patch(facecolor=plt.cm.Set3(0), label='Prompt'),\n",
    "                Patch(facecolor=plt.cm.Set3(1), label='Masked'),\n",
    "                Patch(facecolor=plt.cm.Set3(2), label='Transferred'),\n",
    "                Patch(facecolor=plt.cm.Set3(3), label='Generated')\n",
    "            ]\n",
    "            ax3.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print summary statistics with corrected counts\n",
    "            print(f\"\\nStep Pair {step_pair} Summary:\")\n",
    "            print(f\"  Step {step_a}: Block {block_idx_a}\")\n",
    "            print(f\"    Prompt tokens: {sum(1 for s in token_states_a if s == 'prompt')}\")\n",
    "            print(f\"    Masked tokens: {sum(1 for s in token_states_a if s == 'masked')}\")\n",
    "            print(f\"    Generated tokens: {sum(1 for s in token_states_a if s == 'generated')}\")\n",
    "            print(f\"    Transferred tokens: {sum(1 for s in token_states_a if s == 'transferred')}\")\n",
    "            \n",
    "            print(f\"  Step {step_b}: Block {block_idx_b}\")\n",
    "            print(f\"    Prompt tokens: {sum(1 for s in token_states_b if s == 'prompt')}\")\n",
    "            print(f\"    Masked tokens: {sum(1 for s in token_states_b if s == 'masked')}\")\n",
    "            print(f\"    Generated tokens: {sum(1 for s in token_states_b if s == 'generated')}\")\n",
    "            print(f\"    Transferred tokens: {sum(1 for s in token_states_b if s == 'transferred')}\")\n",
    "            \n",
    "            # Analyze similarity by token type\n",
    "            token_type_similarities = {}\n",
    "            for token_type in ['prompt', 'masked', 'generated', 'transferred']:\n",
    "                type_indices_a = [i for i, state in enumerate(token_states_a) if state == token_type]\n",
    "                type_indices_b = [i for i, state in enumerate(token_states_b) if state == token_type]\n",
    "                common_indices = set(type_indices_a) & set(type_indices_b)\n",
    "                \n",
    "                if common_indices:\n",
    "                    # Get similarity values for this token type\n",
    "                    type_similarities = []\n",
    "                    for token_idx in common_indices:\n",
    "                        if token_idx in group['token_idx'].values:\n",
    "                            token_sim = group[group['token_idx'] == token_idx]['token_similarity'].mean()\n",
    "                            type_similarities.append(token_sim)\n",
    "                    \n",
    "                    if type_similarities:\n",
    "                        token_type_similarities[token_type] = np.mean(type_similarities)\n",
    "            \n",
    "            print(f\"  Average similarity by token type:\")\n",
    "            for token_type, avg_sim in token_type_similarities.items():\n",
    "                print(f\"    {token_type}: {avg_sim:.4f}\")\n",
    "            print()\n",
    "\n",
    "# Run the enhanced analysis with comprehensive token evolution\n",
    "enhanced_cosine_similarity_with_full_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
