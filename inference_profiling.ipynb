{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4162f6e",
   "metadata": {},
   "source": [
    "# LLaDA Inference Profiling on HumanEval\n",
    "\n",
    "This notebook runs inference on the LLaDA model using the HumanEval dataset and collects statistics for profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bd180",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ead82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['HF_HOME'] = '/root/LLaDA/hf_models/'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from generate import generate\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "cache_path = '/root/LLaDA/hf_models/hub'\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, cache_dir=cache_path, local_files_only=True).to(device).eval()\n",
    "\n",
    "# Ensure padding side is left for generation\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf80dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HumanEval Dataset\n",
    "print(\"Loading HumanEval dataset...\")\n",
    "dataset_path = '/root/LLaDA/hf_models/datasets/openai_humaneval'\n",
    "ds = load_dataset(path=dataset_path, split=\"test\")\n",
    "print(f\"Loaded {len(ds)} problems.\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample Problem:\")\n",
    "print(ds[0]['prompt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8e153",
   "metadata": {},
   "source": [
    "## Run Model Inference and Collect Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n",
    "\n",
    "\n",
    "class ActivationProfiler:\n",
    "    def __init__(self, model, target_layers=['k_proj', 'v_proj', 'o_proj'], save_dir='../autodl-tmp/profiling_results'):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.hooks = []\n",
    "        self.save_dir = save_dir\n",
    "        self.current_step = 0\n",
    "        self.buffer = {} # name -> tensor\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def register_hooks(self):\n",
    "        self.clear()\n",
    "        for name, module in self.model.named_modules():\n",
    "            # Check if the module name ends with any of the target layers\n",
    "            if any(name.endswith(t) for t in self.target_layers):\n",
    "                # We only want leaf modules, usually\n",
    "                hook = module.register_forward_hook(self.get_hook(name))\n",
    "                self.hooks.append(hook)\n",
    "        print(f\"Registered hooks on {len(self.hooks)} layers.\")\n",
    "\n",
    "    def get_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            # output shape: [batch, seq_len, hidden_dim]\n",
    "            # Buffer to CPU memory instead of saving immediately to avoid I/O crashes\n",
    "            self.buffer[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    def step(self, current_step_index):\n",
    "        \"\"\"Sets the current step index for saving.\"\"\"\n",
    "        self.current_step = current_step_index\n",
    "\n",
    "    def save_buffer(self):\n",
    "        \"\"\"Saves buffered tensors to disk sequentially.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "            \n",
    "        # Create a directory for the current step\n",
    "        step_dir = os.path.join(self.save_dir, f\"step_{self.current_step}\")\n",
    "        os.makedirs(step_dir, exist_ok=True)\n",
    "        \n",
    "        for name, tensor in self.buffer.items():\n",
    "            # Replace dots with underscores for safe filenames\n",
    "            safe_name = name.replace('.', '_')\n",
    "            file_path = os.path.join(step_dir, f\"{safe_name}.pt\")\n",
    "            torch.save(tensor, file_path)\n",
    "        \n",
    "        # Clear buffer to free memory\n",
    "        self.buffer = {}\n",
    "\n",
    "    def clear(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "        self.buffer = {}\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def get_collected_data(self):\n",
    "        return self.save_dir\n",
    "\n",
    "\n",
    "@ torch.no_grad()\n",
    "def profiled_generate(model, prompt, attention_mask=None, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336, logits_eos_inf=False, confidence_eos_eot_inf=False, profiler=None):\n",
    "    '''\n",
    "    Revised generate function adhering to generate.py logic with profiling.\n",
    "    '''\n",
    "    # Initialize x with mask\n",
    "    x = torch.full((prompt.shape[0], prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((prompt.shape[0], gen_length), dtype=attention_mask.dtype, device=model.device)], dim=-1)\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    # Profiling data storage\n",
    "    step_details = []\n",
    "    \n",
    "    # Print inference statistics\n",
    "    print(f'Generation length: {gen_length}')\n",
    "    print(f'Denoising steps per block: {steps}')\n",
    "    print(f'Number of blocks: {num_blocks}')\n",
    "\n",
    "    # Helper for CUDA synchronization\n",
    "    def synchronize():\n",
    "        if x.device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    synchronize()\n",
    "    total_start = time.perf_counter()\n",
    "    for num_block in range(num_blocks):\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            global_step = num_block * steps + i\n",
    "            if profiler:\n",
    "                # print(f\"flag step: global={global_step} block={num_block} in_block={i}\")\n",
    "                profiler.step(global_step)\n",
    "            \n",
    "            synchronize()\n",
    "            step_start = time.perf_counter()\n",
    "            \n",
    "            mask_index = (x == mask_id)\n",
    "            \n",
    "            # 1. Model Forward Pass\n",
    "            t_forward_start = time.perf_counter()\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask_ = torch.cat([attention_mask, attention_mask], dim=0)\n",
    "                logits = model(x_, attention_mask=attention_mask_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x, attention_mask=attention_mask).logits\n",
    "            synchronize()\n",
    "            t_forward_end = time.perf_counter()\n",
    "\n",
    "            # Save buffered activations for this (global) step\n",
    "            if profiler:\n",
    "                profiler.save_buffer()\n",
    "\n",
    "            if logits_eos_inf:\n",
    "                logits[:, :, 126081] = -torch.inf\n",
    "\n",
    "            # 2. Sampling\n",
    "            t_sample_start = time.perf_counter()\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l\n",
    "            synchronize()\n",
    "            t_sample_end = time.perf_counter()\n",
    "\n",
    "            if confidence_eos_eot_inf:\n",
    "                logits_with_noise[:, :, 126081] = logits[:, :, 126348] = -torch.inf\n",
    "\n",
    "            # 3. Remasking Strategy\n",
    "            t_remask_start = time.perf_counter()\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "            \n",
    "            synchronize()\n",
    "            t_remask_end = time.perf_counter()\n",
    "            \n",
    "            step_end = time.perf_counter()\n",
    "            step_duration = step_end - step_start\n",
    "            \n",
    "            step_details.append({\n",
    "                \"block_idx\": num_block,\n",
    "                \"step_idx\": i,\n",
    "                \"duration\": step_duration,\n",
    "                \"forward_time\": t_forward_end - t_forward_start,\n",
    "                \"sampling_time\": t_sample_end - t_sample_start,\n",
    "                \"remasking_time\": t_remask_end - t_remask_start,\n",
    "                \"num_masks\": mask_index.sum().item()\n",
    "            })\n",
    "\n",
    "    synchronize()\n",
    "    total_end = time.perf_counter()\n",
    "    \n",
    "    return x, step_details\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, prompt_text, steps=64, gen_length=64, block_length=32, profiler=None):\n",
    "    \"\"\"\n",
    "    Runs inference and measures wall time with detailed profiling.\n",
    "    \"\"\"\n",
    "    # Prepare input\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out, step_details = profiled_generate(\n",
    "            model, \n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            steps=steps, \n",
    "            gen_length=gen_length, \n",
    "            block_length=block_length, \n",
    "            temperature=0., \n",
    "            cfg_scale=0., \n",
    "            remasking='low_confidence',\n",
    "            profiler=profiler\n",
    "        )\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text, wall_time, step_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e59ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on a Subset\n",
    "num_samples = 1\n",
    "results = []\n",
    "all_step_details = []\n",
    "\n",
    "# Initialize Profiler\n",
    "tensor_dir = '/root/autodl-tmp/profiling_results'\n",
    "profiler = ActivationProfiler(model, target_layers=['q_proj', 'k_proj', 'v_proj', 'attn_out'], save_dir=tensor_dir)\n",
    "\n",
    "# Ensure hooks are cleared initially\n",
    "profiler.clear()\n",
    "\n",
    "print(f\"Running inference on first {num_samples} samples...\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    problem = ds[i]\n",
    "    prompt = problem['prompt']\n",
    "    task_id = problem['task_id']\n",
    "    \n",
    "    print(f\"Processing {task_id}...\")\n",
    "\n",
    "\t# Register hooks for this run\n",
    "    profiler.register_hooks()\n",
    "\n",
    "    try:\n",
    "        # Using parameters from chat.py/generate.py examples\n",
    "        output, duration, step_details = run_inference(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            prompt_text=prompt, \n",
    "            steps=128, \n",
    "            gen_length=128, \n",
    "            block_length=32,\n",
    "            profiler=profiler\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"wall_time\": duration,\n",
    "            \"output_length\": len(output),\n",
    "            \"output\": output\n",
    "        })\n",
    "        \n",
    "\t\t# Add task_id to each step detail\n",
    "        for d in step_details:\n",
    "            d['task_id'] = task_id\n",
    "        all_step_details.extend(step_details)\n",
    "        print(f\"  Time: {duration:.4f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Remove hooks to free memory and prevent crashes in subsequent runs\n",
    "        profiler.clear()\n",
    "\n",
    "profiler.clear() # Remove hooks after done\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_steps = pd.DataFrame(all_step_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f300f",
   "metadata": {},
   "source": [
    "## Statistics Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93248f7d",
   "metadata": {},
   "source": [
    "### Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Wall-time stats\n",
    "    stats = {\n",
    "        \"Mean Latency\": df_results['wall_time'].mean(),\n",
    "        \"Median Latency\": df_results['wall_time'].median(),\n",
    "        \"Std Dev\": df_results['wall_time'].std(),\n",
    "        \"Min\": df_results['wall_time'].min(),\n",
    "        \"Max\": df_results['wall_time'].max(),\n",
    "        \"P95\": df_results['wall_time'].quantile(0.95),\n",
    "        \"P99\": df_results['wall_time'].quantile(0.99),\n",
    "    }\n",
    "    print(\"Wall Time Statistics (seconds):\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df_results['wall_time'], kde=True, bins=10)\n",
    "    plt.title('Inference Wall Time Distribution')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(df_results['wall_time'].mean(), color='r', linestyle='--', label=f\"Mean: {df_results['wall_time'].mean():.2f}s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Per-step breakdown\n",
    "if not df_steps.empty:\n",
    "    # Totals\n",
    "    totals = df_steps[['forward_time', 'sampling_time', 'remasking_time']].sum()\n",
    "    total_time = totals.sum()\n",
    "    print(\"\\nBreakdown (sum over all steps):\")\n",
    "    for name, val in totals.items():\n",
    "        pct = 100.0 * val / total_time if total_time > 0 else 0.0\n",
    "        print(f\"  {name}: {val:.4f}s ({pct:.1f}%)\")\n",
    "\n",
    "    # Stacked bar per step\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    steps_idx = np.arange(len(df_steps))\n",
    "    fwd = df_steps['forward_time'].values\n",
    "    smp = df_steps['sampling_time'].values\n",
    "    rmk = df_steps['remasking_time'].values\n",
    "    plt.bar(steps_idx, fwd, label='forward')\n",
    "    plt.bar(steps_idx, smp, bottom=fwd, label='sampling')\n",
    "    plt.bar(steps_idx, rmk, bottom=fwd + smp, label='remasking')\n",
    "    plt.title('Per-step Latency Breakdown')\n",
    "    plt.xlabel('Global step')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: aggregate by block\n",
    "    if 'block_idx' in df_steps.columns:\n",
    "        block_agg = df_steps.groupby('block_idx')[['forward_time','sampling_time','remasking_time']].sum()\n",
    "        block_agg.plot(kind='bar', stacked=True, figsize=(8,4), title='Latency Breakdown per Block')\n",
    "        plt.xlabel('Block index')\n",
    "        plt.ylabel('Time (s)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5a365",
   "metadata": {},
   "source": [
    "### Tensor Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d665a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Tensor loader utilities (saved to disk during inference)\n",
    "# =========================\n",
    "import glob, re\n",
    "\n",
    "def load_step_tensors(base_dir, step, selector_substrings):\n",
    "    \"\"\"\n",
    "    Returns {layer_file: tensor} for files containing any selector substring.\n",
    "    selector_substrings: list[str], e.g., ['q_proj', 'k_proj'].\n",
    "    \"\"\"\n",
    "    step_dir = os.path.join(base_dir, f\"step_{step}\")\n",
    "    tensors = {}\n",
    "    if not os.path.isdir(step_dir):\n",
    "        return tensors\n",
    "    for f in glob.glob(os.path.join(step_dir, \"*.pt\")):\n",
    "        base = os.path.basename(f)\n",
    "        if any(s in base for s in selector_substrings):\n",
    "            tensors[base] = torch.load(f, map_location=\"cpu\")\n",
    "    return tensors\n",
    "\n",
    "_layer_re = re.compile(r\".*layers_(\\d+).*_(q_proj|k_proj|v_proj|attn_out|ff_proj|up_proj|ff_out)\\.pt$\")\n",
    "\n",
    "def parse_layer_and_kind(layer_file_basename):\n",
    "    \"\"\"\n",
    "    Extract (layer_idx:int, kind:str) from filename like 'model_layers_12_self_attn_q_proj.pt'.\n",
    "    Returns (None, kind) if layer index not found.\n",
    "    \"\"\"\n",
    "    m = _layer_re.match(layer_file_basename)\n",
    "    if m:\n",
    "        return int(m.group(1)), m.group(2)\n",
    "    # Fallback: detect kind only\n",
    "    for k in ['q_proj','k_proj','v_proj','attn_out','ff_proj','up_proj','ff_out']:\n",
    "        if k in layer_file_basename:\n",
    "            return None, k\n",
    "    return None, None\n",
    "\n",
    "def reduce_stat(t, how=\"mean_abs\", sample=None):\n",
    "    \"\"\"\n",
    "    Reduce a tensor [batch, seq, hidden] to a scalar.\n",
    "    how: 'mean_abs' | 'max_abs' | 'std' | 'l2'\n",
    "    sample: dict to subsample dims, e.g., {'seq': 256, 'hidden': 512}\n",
    "    \"\"\"\n",
    "    x = t\n",
    "    # optional subsample to reduce memory/plot time\n",
    "    if sample:\n",
    "        b, s, h = x.shape\n",
    "        if 'seq' in sample:\n",
    "            x = x[:, torch.linspace(0, s-1, steps=min(sample['seq'], s), dtype=torch.long), :]\n",
    "        if 'hidden' in sample:\n",
    "            x = x[:, :, torch.linspace(0, h-1, steps=min(sample['hidden'], h), dtype=torch.long)]\n",
    "    x = x.float()\n",
    "    if how == \"mean_abs\":\n",
    "        return x.abs().mean().item()\n",
    "    if how == \"max_abs\":\n",
    "        return x.abs().amax().item()\n",
    "    if how == \"std\":\n",
    "        return x.std().item()\n",
    "    if how == \"l2\":\n",
    "        return x.pow(2).mean().sqrt().item()\n",
    "    raise ValueError(how)\n",
    "\n",
    "def collect_series(base_dir, selectors=('q_proj','k_proj'), how=\"mean_abs\", sample=None, steps=None):\n",
    "    \"\"\"\n",
    "    Scan steps->layers and compute a scalar stat per (step, layer, kind).\n",
    "    Returns DataFrame columns: step, layer_idx, kind, value\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # infer steps if not provided\n",
    "    if steps is None:\n",
    "        step_dirs = sorted([d for d in os.listdir(base_dir) if d.startswith(\"step_\")],\n",
    "                           key=lambda x: int(x.split('_')[1]))\n",
    "        steps = [int(d.split('_')[1]) for d in step_dirs]\n",
    "    for step in steps:\n",
    "        tensors = load_step_tensors(base_dir, step, selectors)\n",
    "        for name, t in tensors.items():\n",
    "            layer_idx, kind = parse_layer_and_kind(name)\n",
    "            if kind is None: \n",
    "                continue\n",
    "            val = reduce_stat(t, how=how, sample=sample)\n",
    "            rows.append({\"step\": step, \"layer_idx\": layer_idx, \"kind\": kind, \"value\": val, \"name\": name})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# Plots: evolution across layers and denoise steps\n",
    "# =========================\n",
    "# Example 1: Heatmap of mean_abs over (steps x layers) for q_proj\n",
    "df_q = collect_series(tensor_dir, selectors=('q_proj',), how=\"mean_abs\", sample={\"seq\":256, \"hidden\":512})\n",
    "if not df_q.empty and df_q['layer_idx'].notna().all():\n",
    "    pivot = df_q.pivot_table(index=\"step\", columns=\"layer_idx\", values=\"value\", aggfunc=\"mean\").sort_index()\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.heatmap(pivot.T, cmap=\"viridis\", cbar_kws={\"label\": \"mean|x|\"}, robust=True)\n",
    "    plt.title(\"q_proj mean_abs across layers (cols) and steps (rows)\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Layer index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 2: Compare q_proj vs k_proj averaged over layers per step\n",
    "df_qk = collect_series(tensor_dir, selectors=('q_proj','k_proj'), how=\"max_abs\", sample={\"seq\":256, \"hidden\":512})\n",
    "if not df_qk.empty:\n",
    "    df_avg = df_qk.groupby(['step','kind'])['value'].mean().reset_index()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.lineplot(data=df_avg, x='step', y='value', hue='kind')\n",
    "    plt.title(\"q_proj vs k_proj (avg over layers) â€” max_abs\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"max|x|\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 3: Distribution at a given step for specific layers/kind\n",
    "TARGET_STEP = df_q['step'].max() if not df_q.empty else None\n",
    "if TARGET_STEP is not None:\n",
    "    tensors = load_step_tensors(tensor_dir, TARGET_STEP, ['q_proj'])\n",
    "    # pick first layer entry for demo\n",
    "    if tensors:\n",
    "        name, tensor = sorted(tensors.items())[0]\n",
    "        x = tensor.float().view(-1).numpy()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.histplot(x, bins=100, kde=True)\n",
    "        plt.title(f\"Distribution of {name} at step={TARGET_STEP}\")\n",
    "        plt.xlabel(\"Activation value\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
